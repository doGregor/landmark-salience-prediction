{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from image_data_module import TrainTestData\n",
    "from feature_extraction_module import FeatureExtractor\n",
    "from salience_prediction_module import SaliencePrediction\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = TrainTestData()\n",
    "extraction_module = FeatureExtractor()\n",
    "prediction_module = SaliencePrediction()\n",
    "\n",
    "def get_relevant_elements(all_values, relevant_values):\n",
    "    elements = []\n",
    "    for value in relevant_values:\n",
    "        elements.append(np.where(all_values == value)[0][0])\n",
    "    return np.asarray(elements)\n",
    "\n",
    "random_pred_mape = np.asarray([23.56, 29.23, 22.95, 26.77, 23.93])\n",
    "random_pred_accuracy = np.asarray([0.48, 0.528, 0.512, 0.548, 0.469])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for cv split 0\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "(1014, 274) (252, 274)\n",
      "Preparing data for cv split 1\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "(1014, 274) (252, 274)\n",
      "Preparing data for cv split 2\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "(1014, 274) (252, 274)\n",
      "Preparing data for cv split 3\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "(1014, 274) (252, 274)\n",
      "Preparing data for cv split 4\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "(1008, 274) (258, 274)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "style_file = open(r\"learning_output/vgg_style.pickle\", \"rb\")\n",
    "style_dict_results = pickle.load(style_file)\n",
    "style_gram_matrices = style_dict_results[\"gram_matrices\"]\n",
    "style_image_ids = style_dict_results[\"image_ids\"]\n",
    "\n",
    "content_file = open(r\"learning_output/vgg_content.pickle\", \"rb\")\n",
    "content_dict_results = pickle.load(content_file)\n",
    "content_gram_matrices = content_dict_results[\"gram_matrices\"]\n",
    "content_image_ids = content_dict_results[\"image_ids\"]\n",
    "\n",
    "data_dict = {}\n",
    "for cv_id in range(0, 5):\n",
    "    print(\"Preparing data for cv split\", str(cv_id))\n",
    "    (X_train_ids, X_test_ids) = data_module.get_train_test_image_ids(str(cv_id))\n",
    "    \n",
    "    ### STYLE MATRICES ###\n",
    "    style_X_train = style_gram_matrices[get_relevant_elements(style_image_ids, X_train_ids)]\n",
    "    style_X_test = style_gram_matrices[get_relevant_elements(style_image_ids, X_test_ids)]\n",
    "    \n",
    "    style_X_train, style_X_test = extraction_module.PCA(style_X_train, style_X_test, components=128, save_fig=False, save_model=False)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    style_X_train = scaler.fit_transform(style_X_train)\n",
    "    style_X_test = scaler.transform(style_X_test)\n",
    "    \n",
    "    ### CONTENT MATRICES ###\n",
    "    content_X_train = content_gram_matrices[get_relevant_elements(content_image_ids, X_train_ids)]\n",
    "    content_X_test = content_gram_matrices[get_relevant_elements(content_image_ids, X_test_ids)]\n",
    "    \n",
    "    content_X_train, content_X_test = extraction_module.PCA(content_X_train, content_X_test, components=128, save_fig=False, save_model=False)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    content_X_train = scaler.fit_transform(content_X_train)\n",
    "    content_X_test = scaler.transform(content_X_test)\n",
    "    \n",
    "    ### COMPLEXITY ###\n",
    "    (X_train, Y_train), (X_test, Y_test) = data_module.get_train_test_salience(cv_name=str(cv_id), gray=True)\n",
    "    complexity_X_train = extraction_module.complexity(X_train, mode='grid')\n",
    "    complexity_X_test = extraction_module.complexity(X_test, mode='grid')\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    complexity_X_train = scaler.fit_transform(complexity_X_train)\n",
    "    complexity_X_test = scaler.transform(complexity_X_test)\n",
    "    \n",
    "    ### MERGE DATA ###\n",
    "    X_train = np.hstack((style_X_train, content_X_train, complexity_X_train))\n",
    "    X_test = np.hstack((style_X_test, content_X_test, complexity_X_test))\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    \n",
    "    ### WRITE DATA ###\n",
    "    data_dict[str(cv_id)] = [X_train, X_test]\n",
    "    \n",
    "### SAVE DATA ###\n",
    "file_path = 'learning_output/representation.pickle'\n",
    "with open(file_path, \"wb\") as output_file:\n",
    "    pickle.dump(data_dict, output_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cv split 0\n",
      "1014/1014 [==============================] - 0s 63us/sample - loss: 0.0657 - mse: 0.0089 - mae: 0.0658 - mape: 13.6119\n",
      "train loss, train mse, train mae, train mape: [0.06574999248605273, 0.008876002, 0.06575, 13.611857]\n",
      "252/252 [==============================] - 0s 74us/sample - loss: 0.0747 - mse: 0.0093 - mae: 0.0747 - mape: 14.3351\n",
      "test loss, test mse, test mae, test mape: [0.07474197494605231, 0.00932798, 0.074741974, 14.3351]\n",
      "Evaluating cv split 1\n",
      "1014/1014 [==============================] - 0s 47us/sample - loss: 0.0646 - mse: 0.0081 - mae: 0.0646 - mape: 12.9611\n",
      "train loss, train mse, train mae, train mape: [0.06461104021740148, 0.00814012, 0.06461103, 12.961073]\n",
      "252/252 [==============================] - 0s 71us/sample - loss: 0.0908 - mse: 0.0155 - mae: 0.0908 - mape: 20.8932\n",
      "test loss, test mse, test mae, test mape: [0.09083013603138546, 0.015517354, 0.09083014, 20.893173]\n",
      "Evaluating cv split 2\n",
      "1014/1014 [==============================] - 0s 69us/sample - loss: 0.0616 - mse: 0.0077 - mae: 0.0616 - mape: 13.1549\n",
      "train loss, train mse, train mae, train mape: [0.06158139753565045, 0.0077272113, 0.061581396, 13.154864]\n",
      "252/252 [==============================] - 0s 72us/sample - loss: 0.0920 - mse: 0.0135 - mae: 0.0920 - mape: 17.1420\n",
      "test loss, test mse, test mae, test mape: [0.09203517472460157, 0.013470192, 0.092035174, 17.142015]\n",
      "Evaluating cv split 3\n",
      "1014/1014 [==============================] - 0s 61us/sample - loss: 0.0548 - mse: 0.0068 - mae: 0.0548 - mape: 11.2342\n",
      "train loss, train mse, train mae, train mape: [0.05483859221963487, 0.006766415, 0.05483859, 11.234181]\n",
      "252/252 [==============================] - 0s 70us/sample - loss: 0.0944 - mse: 0.0152 - mae: 0.0944 - mape: 20.5593\n",
      "test loss, test mse, test mae, test mape: [0.09435730072714034, 0.015180943, 0.094357304, 20.55926]\n",
      "Evaluating cv split 4\n",
      "1008/1008 [==============================] - 0s 63us/sample - loss: 0.0680 - mse: 0.0091 - mae: 0.0680 - mape: 14.1331\n",
      "train loss, train mse, train mae, train mape: [0.0680294054604712, 0.0091203805, 0.068029396, 14.133145]\n",
      "258/258 [==============================] - 0s 73us/sample - loss: 0.0756 - mse: 0.0099 - mae: 0.0756 - mape: 13.8765\n",
      "test loss, test mse, test mae, test mape: [0.07557935793270436, 0.009869554, 0.07557935, 13.876527]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### REGRESSION ###\n",
    "\n",
    "data_file = open(r\"learning_output/representation.pickle\", \"rb\")\n",
    "data_dict = pickle.load(data_file)\n",
    "\n",
    "for cv_id in range(0, 5):\n",
    "    print(\"Evaluating cv split\", str(cv_id))\n",
    "    \n",
    "    X_train = data_dict[str(cv_id)][0]\n",
    "    X_test = data_dict[str(cv_id)][1]\n",
    "    \n",
    "    Y_train, Y_test = data_module.get_salience_only(str(cv_id))\n",
    "    Y_train, Y_test = Y_train / 5.0, Y_test / 5.0\n",
    "    \n",
    "    save_name = \"dnn_regression_\" + str(cv_id)\n",
    "    model = prediction_module.initialize_dnn_regression(input_shape=(X_train.shape[1],), learning_rate=0.0004, summary=False)\n",
    "    prediction_module.train_dnn_regression(model, X_train, Y_train, X_test, Y_test, epochs=1000, batch_size=32, verbose=0,\n",
    "                                           evaluate=True, plot=True, save_name=save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cv split 0\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "1014/1014 [==============================] - 0s 107us/sample - loss: 0.4925 - accuracy: 0.8146\n",
      "train loss, train acc: [0.4925375216341113, 0.81459564]\n",
      "252/252 [==============================] - 0s 53us/sample - loss: 0.6649 - accuracy: 0.5952\n",
      "test loss, test acc: [0.664867173111628, 0.5952381]\n",
      "Evaluating cv split 1\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "1014/1014 [==============================] - 0s 118us/sample - loss: 0.5000 - accuracy: 0.8018\n",
      "train loss, train acc: [0.5000206291440441, 0.80177516]\n",
      "252/252 [==============================] - 0s 63us/sample - loss: 0.6345 - accuracy: 0.6389\n",
      "test loss, test acc: [0.6345005063783555, 0.6388889]\n",
      "Evaluating cv split 2\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "1014/1014 [==============================] - 0s 109us/sample - loss: 0.4951 - accuracy: 0.8136\n",
      "train loss, train acc: [0.49506891349833865, 0.8136095]\n",
      "252/252 [==============================] - 0s 51us/sample - loss: 0.6656 - accuracy: 0.6071\n",
      "test loss, test acc: [0.6655937045339554, 0.60714287]\n",
      "Evaluating cv split 3\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "1014/1014 [==============================] - 0s 113us/sample - loss: 0.5017 - accuracy: 0.8077\n",
      "train loss, train acc: [0.5017011164443262, 0.8076923]\n",
      "252/252 [==============================] - 0s 59us/sample - loss: 0.6661 - accuracy: 0.6032\n",
      "test loss, test acc: [0.6660820917477683, 0.6031746]\n",
      "Evaluating cv split 4\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "1008/1008 [==============================] - 0s 107us/sample - loss: 0.4779 - accuracy: 0.8175\n",
      "train loss, train acc: [0.4778894583384196, 0.8174603]\n",
      "258/258 [==============================] - 0s 63us/sample - loss: 0.6807 - accuracy: 0.5969\n",
      "test loss, test acc: [0.6807050982186961, 0.5968992]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### CLASSIFICATION ###\n",
    "\n",
    "data_file = open(r\"learning_output/representation.pickle\", \"rb\")\n",
    "data_dict = pickle.load(data_file)\n",
    "\n",
    "for cv_id in range(0, 5):\n",
    "    print(\"Evaluating cv split\", str(cv_id))\n",
    "    \n",
    "    X_train = data_dict[str(cv_id)][0]\n",
    "    X_test = data_dict[str(cv_id)][1]\n",
    "    \n",
    "    Y_train, Y_test = data_module.get_binary_only(str(cv_id))\n",
    "    class_weights = prediction_module.compute_class_weights(Y_train)\n",
    "\n",
    "    # adam, 0.00002, 32, 600\n",
    "    \n",
    "    save_name = \"dnn_classification_\" + str(cv_id)\n",
    "    model = prediction_module.initialize_dnn_classification(input_shape=(X_train.shape[1],), learning_rate=0.00002, summary=False)\n",
    "    \n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 100:\n",
    "            return lr\n",
    "        elif epoch < 250:\n",
    "            return 0.000015\n",
    "        elif epoch < 400:\n",
    "            return 0.00001\n",
    "        else:\n",
    "            return 0.000005\n",
    "    callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    \n",
    "    prediction_module.train_dnn_classification(model, X_train, Y_train, X_test, Y_test, epochs=600, batch_size=32,\n",
    "                                               verbose=0, class_weights=class_weights,\n",
    "                                               evaluate=True, plot=True, save_name=save_name, callback=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cv split 0\n",
      "[INFO] Starting Random Forest Fitting and Prediction\n",
      "[INFO] Finished Random Forest Fitting and Prediction\n",
      "{'accuracy': 0.6071428571428571}\n",
      "Evaluating cv split 1\n",
      "[INFO] Starting Random Forest Fitting and Prediction\n",
      "[INFO] Finished Random Forest Fitting and Prediction\n",
      "{'accuracy': 0.6428571428571429}\n",
      "Evaluating cv split 2\n",
      "[INFO] Starting Random Forest Fitting and Prediction\n",
      "[INFO] Finished Random Forest Fitting and Prediction\n",
      "{'accuracy': 0.6388888888888888}\n",
      "Evaluating cv split 3\n",
      "[INFO] Starting Random Forest Fitting and Prediction\n",
      "[INFO] Finished Random Forest Fitting and Prediction\n",
      "{'accuracy': 0.6468253968253969}\n",
      "Evaluating cv split 4\n",
      "[INFO] Starting Random Forest Fitting and Prediction\n",
      "[INFO] Finished Random Forest Fitting and Prediction\n",
      "{'accuracy': 0.6007751937984496}\n",
      "AVG ACCURACY: 0.627297895902547\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFICATION TEST ###\n",
    "\n",
    "acc_all = []\n",
    "\n",
    "data_file = open(r\"learning_output/representation.pickle\", \"rb\")\n",
    "data_dict = pickle.load(data_file)\n",
    "\n",
    "for cv_id in range(0, 5):\n",
    "    print(\"Evaluating cv split\", str(cv_id))\n",
    "    \n",
    "    X_train = data_dict[str(cv_id)][0]\n",
    "    X_test = data_dict[str(cv_id)][1]\n",
    "    \n",
    "    Y_train, Y_test = data_module.get_binary_only(str(cv_id))\n",
    "    \n",
    "    result = prediction_module.feature_trend(X_train, Y_train, X_test, Y_test)\n",
    "    print(result)\n",
    "    acc_all.append(result[\"accuracy\"])\n",
    "    \n",
    "print(\"AVG ACCURACY:\", sum(acc_all)/len(acc_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
