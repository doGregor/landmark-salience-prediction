{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from image_data_module import TrainTestData\n",
    "from feature_extraction_module import FeatureExtractor\n",
    "from salience_prediction_module import SaliencePrediction\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = TrainTestData()\n",
    "extraction_module = FeatureExtractor()\n",
    "prediction_module = SaliencePrediction()\n",
    "\n",
    "def get_relevant_elements(all_values, relevant_values):\n",
    "    elements = []\n",
    "    for value in relevant_values:\n",
    "        elements.append(np.where(all_values == value)[0][0])\n",
    "    return np.asarray(elements)\n",
    "\n",
    "random_pred_mape = np.asarray([23.56, 29.23, 22.95, 26.77, 23.93])\n",
    "random_pred_accuracy = np.asarray([0.48, 0.528, 0.512, 0.548, 0.469])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for cv split 0\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "(1014, 274) (252, 274)\n",
      "Preparing data for cv split 1\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "(1014, 274) (252, 274)\n",
      "Preparing data for cv split 2\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "(1014, 274) (252, 274)\n",
      "Preparing data for cv split 3\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "(1014, 274) (252, 274)\n",
      "Preparing data for cv split 4\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting PCA\n",
      "[INFO] Finished PCA\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "[INFO] Starting Complexity Computation\n",
      "[INFO] Starting Sobel Detection\n",
      "[INFO] Finished Sobel Detection\n",
      "[INFO] Finished Complexity Computation\n",
      "(1008, 274) (258, 274)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "style_file = open(r\"learning_output/vgg_style.pickle\", \"rb\")\n",
    "style_dict_results = pickle.load(style_file)\n",
    "style_gram_matrices = style_dict_results[\"gram_matrices\"]\n",
    "style_image_ids = style_dict_results[\"image_ids\"]\n",
    "\n",
    "content_file = open(r\"learning_output/vgg_content.pickle\", \"rb\")\n",
    "content_dict_results = pickle.load(content_file)\n",
    "content_gram_matrices = content_dict_results[\"gram_matrices\"]\n",
    "content_image_ids = content_dict_results[\"image_ids\"]\n",
    "\n",
    "data_dict = {}\n",
    "for cv_id in range(0, 5):\n",
    "    print(\"Preparing data for cv split\", str(cv_id))\n",
    "    (X_train_ids, X_test_ids) = data_module.get_train_test_image_ids(str(cv_id))\n",
    "    \n",
    "    ### STYLE MATRICES ###\n",
    "    style_X_train = style_gram_matrices[get_relevant_elements(style_image_ids, X_train_ids)]\n",
    "    style_X_test = style_gram_matrices[get_relevant_elements(style_image_ids, X_test_ids)]\n",
    "    \n",
    "    style_X_train, style_X_test = extraction_module.PCA(style_X_train, style_X_test, components=128, save_fig=False, save_model=False)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    style_X_train = scaler.fit_transform(style_X_train)\n",
    "    style_X_test = scaler.transform(style_X_test)\n",
    "    \n",
    "    ### CONTENT MATRICES ###\n",
    "    content_X_train = content_gram_matrices[get_relevant_elements(content_image_ids, X_train_ids)]\n",
    "    content_X_test = content_gram_matrices[get_relevant_elements(content_image_ids, X_test_ids)]\n",
    "    \n",
    "    content_X_train, content_X_test = extraction_module.PCA(content_X_train, content_X_test, components=128, save_fig=False, save_model=False)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    content_X_train = scaler.fit_transform(content_X_train)\n",
    "    content_X_test = scaler.transform(content_X_test)\n",
    "    \n",
    "    ### COMPLEXITY ###\n",
    "    (X_train, Y_train), (X_test, Y_test) = data_module.get_train_test_salience(cv_name=str(cv_id), gray=True)\n",
    "    complexity_X_train = extraction_module.complexity(X_train, mode='grid')\n",
    "    complexity_X_test = extraction_module.complexity(X_test, mode='grid')\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    complexity_X_train = scaler.fit_transform(complexity_X_train)\n",
    "    complexity_X_test = scaler.transform(complexity_X_test)\n",
    "    \n",
    "    ### MERGE DATA ###\n",
    "    X_train = np.hstack((style_X_train, content_X_train, complexity_X_train))\n",
    "    X_test = np.hstack((style_X_test, content_X_test, complexity_X_test))\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    \n",
    "    ### WRITE DATA ###\n",
    "    data_dict[str(cv_id)] = [X_train, X_test]\n",
    "    \n",
    "### SAVE DATA ###\n",
    "file_path = 'learning_output/representation.pickle'\n",
    "with open(file_path, \"wb\") as output_file:\n",
    "    pickle.dump(data_dict, output_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cv split 0\n",
      "1014/1014 [==============================] - 0s 58us/sample - loss: 0.0663 - mse: 0.0086 - mae: 0.0663 - mape: 13.9347\n",
      "train loss, train mse, train mae, train mape: [0.06627044645992257, 0.008616955, 0.06627045, 13.934746]\n",
      "252/252 [==============================] - 0s 61us/sample - loss: 0.0730 - mse: 0.0088 - mae: 0.0730 - mape: 14.2098\n",
      "test loss, test mse, test mae, test mape: [0.07296174126011985, 0.008830873, 0.07296175, 14.209819]\n",
      "Evaluating cv split 1\n",
      "1014/1014 [==============================] - 0s 64us/sample - loss: 0.0624 - mse: 0.0078 - mae: 0.0624 - mape: 12.5533\n",
      "train loss, train mse, train mae, train mape: [0.06237753831893499, 0.007781198, 0.062377542, 12.55329]\n",
      "252/252 [==============================] - 0s 73us/sample - loss: 0.0936 - mse: 0.0163 - mae: 0.0936 - mape: 21.3867\n",
      "test loss, test mse, test mae, test mape: [0.09355804643460683, 0.016284583, 0.09355805, 21.386692]\n",
      "Evaluating cv split 2\n",
      "1014/1014 [==============================] - 0s 56us/sample - loss: 0.0567 - mse: 0.0071 - mae: 0.0567 - mape: 12.0909\n",
      "train loss, train mse, train mae, train mape: [0.05671955314437313, 0.0071218456, 0.05671955, 12.090855]\n",
      "252/252 [==============================] - 0s 60us/sample - loss: 0.0941 - mse: 0.0144 - mae: 0.0941 - mape: 17.5505\n",
      "test loss, test mse, test mae, test mape: [0.09408641168995509, 0.014398588, 0.09408642, 17.550488]\n",
      "Evaluating cv split 3\n",
      "1014/1014 [==============================] - 0s 57us/sample - loss: 0.0720 - mse: 0.0095 - mae: 0.0720 - mape: 14.4851\n",
      "train loss, train mse, train mae, train mape: [0.07203111681477323, 0.009508703, 0.07203111, 14.485124]\n",
      "252/252 [==============================] - 0s 80us/sample - loss: 0.0913 - mse: 0.0146 - mae: 0.0913 - mape: 20.0814\n",
      "test loss, test mse, test mae, test mape: [0.09125427724350066, 0.014628561, 0.09125428, 20.081385]\n",
      "Evaluating cv split 4\n",
      "1008/1008 [==============================] - 0s 57us/sample - loss: 0.0606 - mse: 0.0079 - mae: 0.0606 - mape: 12.7623\n",
      "train loss, train mse, train mae, train mape: [0.060616052339947414, 0.007928618, 0.060616054, 12.762329]\n",
      "258/258 [==============================] - 0s 67us/sample - loss: 0.0861 - mse: 0.0121 - mae: 0.0861 - mape: 15.7664\n",
      "test loss, test mse, test mae, test mape: [0.08608696534652119, 0.012061265, 0.08608696, 15.766444]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### REGRESSION ###\n",
    "\n",
    "data_file = open(r\"learning_output/representation.pickle\", \"rb\")\n",
    "data_dict = pickle.load(data_file)\n",
    "\n",
    "for cv_id in range(0, 5):\n",
    "    print(\"Evaluating cv split\", str(cv_id))\n",
    "    \n",
    "    X_train = data_dict[str(cv_id)][0]\n",
    "    X_test = data_dict[str(cv_id)][1]\n",
    "    \n",
    "    Y_train, Y_test = data_module.get_salience_only(str(cv_id))\n",
    "    Y_train, Y_test = Y_train / 5.0, Y_test / 5.0\n",
    "    \n",
    "    save_name = \"dnn_regression_\" + str(cv_id)\n",
    "    model = prediction_module.initialize_dnn_regression(input_shape=(X_train.shape[1],), learning_rate=0.0004, summary=False)\n",
    "    prediction_module.train_dnn_regression(model, X_train, Y_train, X_test, Y_test, epochs=1000, batch_size=32, verbose=0,\n",
    "                                           evaluate=True, plot=True, save_name=save_name, save=True, delete=False)\n",
    "    \n",
    "    train_ids, test_ids = data_module.get_train_test_image_ids(str(cv_id))\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    with open('predictions.txt', 'a') as csv_file:\n",
    "        for idx, value in enumerate(test_ids):\n",
    "            line = str(value) + \", \" + str(Y_test[idx]) + \", \" + str(predictions[idx][0]) + \", \" + str(abs(Y_test[idx] - predictions[idx][0])) + \", \" + str(abs((Y_test[idx] - predictions[idx][0]) / Y_test[idx])*100) + \"\\n\"\n",
    "            csv_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cv split 0\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "1014/1014 [==============================] - 0s 107us/sample - loss: 0.4925 - accuracy: 0.8146\n",
      "train loss, train acc: [0.4925375216341113, 0.81459564]\n",
      "252/252 [==============================] - 0s 53us/sample - loss: 0.6649 - accuracy: 0.5952\n",
      "test loss, test acc: [0.664867173111628, 0.5952381]\n",
      "Evaluating cv split 1\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "1014/1014 [==============================] - 0s 118us/sample - loss: 0.5000 - accuracy: 0.8018\n",
      "train loss, train acc: [0.5000206291440441, 0.80177516]\n",
      "252/252 [==============================] - 0s 63us/sample - loss: 0.6345 - accuracy: 0.6389\n",
      "test loss, test acc: [0.6345005063783555, 0.6388889]\n",
      "Evaluating cv split 2\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "1014/1014 [==============================] - 0s 109us/sample - loss: 0.4951 - accuracy: 0.8136\n",
      "train loss, train acc: [0.49506891349833865, 0.8136095]\n",
      "252/252 [==============================] - 0s 51us/sample - loss: 0.6656 - accuracy: 0.6071\n",
      "test loss, test acc: [0.6655937045339554, 0.60714287]\n",
      "Evaluating cv split 3\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "1014/1014 [==============================] - 0s 113us/sample - loss: 0.5017 - accuracy: 0.8077\n",
      "train loss, train acc: [0.5017011164443262, 0.8076923]\n",
      "252/252 [==============================] - 0s 59us/sample - loss: 0.6661 - accuracy: 0.6032\n",
      "test loss, test acc: [0.6660820917477683, 0.6031746]\n",
      "Evaluating cv split 4\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "1008/1008 [==============================] - 0s 107us/sample - loss: 0.4779 - accuracy: 0.8175\n",
      "train loss, train acc: [0.4778894583384196, 0.8174603]\n",
      "258/258 [==============================] - 0s 63us/sample - loss: 0.6807 - accuracy: 0.5969\n",
      "test loss, test acc: [0.6807050982186961, 0.5968992]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### CLASSIFICATION ###\n",
    "\n",
    "data_file = open(r\"learning_output/representation.pickle\", \"rb\")\n",
    "data_dict = pickle.load(data_file)\n",
    "\n",
    "for cv_id in range(0, 5):\n",
    "    print(\"Evaluating cv split\", str(cv_id))\n",
    "    \n",
    "    X_train = data_dict[str(cv_id)][0]\n",
    "    X_test = data_dict[str(cv_id)][1]\n",
    "    \n",
    "    Y_train, Y_test = data_module.get_binary_only(str(cv_id))\n",
    "    class_weights = prediction_module.compute_class_weights(Y_train)\n",
    "\n",
    "    # adam, 0.00002, 32, 600\n",
    "    \n",
    "    save_name = \"dnn_classification_\" + str(cv_id)\n",
    "    model = prediction_module.initialize_dnn_classification(input_shape=(X_train.shape[1],), learning_rate=0.00002, summary=False)\n",
    "    \n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 100:\n",
    "            return lr\n",
    "        elif epoch < 250:\n",
    "            return 0.000015\n",
    "        elif epoch < 400:\n",
    "            return 0.00001\n",
    "        else:\n",
    "            return 0.000005\n",
    "    callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    \n",
    "    prediction_module.train_dnn_classification(model, X_train, Y_train, X_test, Y_test, epochs=600, batch_size=32,\n",
    "                                               verbose=0, class_weights=class_weights,\n",
    "                                               evaluate=True, plot=True, save_name=save_name, callback=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating cv split 0\n",
      "[INFO] Starting Random Forest Fitting and Prediction\n",
      "[INFO] Finished Random Forest Fitting and Prediction\n",
      "{'accuracy': 0.6071428571428571}\n",
      "Evaluating cv split 1\n",
      "[INFO] Starting Random Forest Fitting and Prediction\n",
      "[INFO] Finished Random Forest Fitting and Prediction\n",
      "{'accuracy': 0.6428571428571429}\n",
      "Evaluating cv split 2\n",
      "[INFO] Starting Random Forest Fitting and Prediction\n",
      "[INFO] Finished Random Forest Fitting and Prediction\n",
      "{'accuracy': 0.6388888888888888}\n",
      "Evaluating cv split 3\n",
      "[INFO] Starting Random Forest Fitting and Prediction\n",
      "[INFO] Finished Random Forest Fitting and Prediction\n",
      "{'accuracy': 0.6468253968253969}\n",
      "Evaluating cv split 4\n",
      "[INFO] Starting Random Forest Fitting and Prediction\n",
      "[INFO] Finished Random Forest Fitting and Prediction\n",
      "{'accuracy': 0.6007751937984496}\n",
      "AVG ACCURACY: 0.627297895902547\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFICATION TEST ###\n",
    "\n",
    "acc_all = []\n",
    "\n",
    "data_file = open(r\"learning_output/representation.pickle\", \"rb\")\n",
    "data_dict = pickle.load(data_file)\n",
    "\n",
    "for cv_id in range(0, 5):\n",
    "    print(\"Evaluating cv split\", str(cv_id))\n",
    "    \n",
    "    X_train = data_dict[str(cv_id)][0]\n",
    "    X_test = data_dict[str(cv_id)][1]\n",
    "    \n",
    "    Y_train, Y_test = data_module.get_binary_only(str(cv_id))\n",
    "    \n",
    "    result = prediction_module.feature_trend(X_train, Y_train, X_test, Y_test)\n",
    "    print(result)\n",
    "    acc_all.append(result[\"accuracy\"])\n",
    "    \n",
    "print(\"AVG ACCURACY:\", sum(acc_all)/len(acc_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
